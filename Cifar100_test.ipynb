{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cifar100-test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utsavsinghal5/cifar100/blob/master/Cifar100_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwzxN_VFKO1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sys import argv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, BatchNormalization,Dropout\n",
        "import keras\n",
        "from keras import regularizers\n",
        "\n",
        "\n",
        "def lr_value(epoch):\n",
        "    if(epoch < 75):\n",
        "        return 0.001\n",
        "    if(epoch < 100):\n",
        "        return 0.0005\n",
        "    return 0.0002\n",
        "\n",
        "# Take input of data\n",
        "train = np.array(pd.read_csv('/kaggle/input/cifar10flattened/train.csv',header=None,delim_whitespace=True))\n",
        "test = np.array(pd.read_csv('/kaggle/input/cifar10flattened/test.csv',header=None,delim_whitespace=True))\n",
        "\n",
        "# Process data\n",
        "Xtr = train[:,:-1]\n",
        "Xtr = Xtr.reshape((Xtr.shape[0], 3, 32, 32))\n",
        "Ytr = train[:,-1].reshape((Xtr.shape[0],1))\n",
        "Xtest = test[:,:-1]\n",
        "Xtest = Xtest.reshape((Xtest.shape[0],3, 32, 32))\n",
        "weight_decay = 1e-4\n",
        "# below is one hot encoding\n",
        "\n",
        "Xtr = Xtr.astype('float32')\n",
        "Xtest = Xtest.astype('float32')\n",
        "\n",
        "Xtr = (Xtr- np.mean(Xtr))/np.std(Xtr)\n",
        "Xtest = (Xtest- np.mean(Xtest))/np.std(Xtest)\n",
        "\n",
        "\n",
        "Ytr = keras.utils.to_categorical(Ytr,10)\n",
        "\n",
        "# developing design for model\n",
        "cnn_model = Sequential()\n",
        "cnn_model.add(Conv2D(filters=64,kernel_size= (3,3), padding = 'same', activation = 'elu', data_format='channels_first',input_shape=(3, 32,32),use_bias = True, kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "cnn_model.add(BatchNormalization())\n",
        "cnn_model.add(Conv2D(filters=128,kernel_size= (3,3), padding = 'same', activation = 'elu', data_format='channels_first',input_shape=(3, 32,32), kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "cnn_model.add(BatchNormalization())\n",
        "cnn_model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "cnn_model.add(Dropout(0.35))\n",
        "\n",
        "cnn_model.add(Conv2D(filters=128,kernel_size= (3,3), padding = 'same', activation = 'elu', data_format='channels_first',input_shape=(3, 32,32),use_bias = True, kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "cnn_model.add(BatchNormalization())\n",
        "cnn_model.add(Conv2D(filters=128,kernel_size= (3,3), padding = 'same', activation = 'elu', data_format='channels_first',input_shape=(3, 32,32),use_bias = True, kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "cnn_model.add(BatchNormalization())\n",
        "cnn_model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "cnn_model.add(Dropout(0.5))\n",
        "\n",
        "cnn_model.add(Conv2D(filters=128,kernel_size= (3,3), padding = 'same', activation = 'elu', data_format='channels_first',input_shape=(3, 32,32),use_bias = True, kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "cnn_model.add(BatchNormalization())\n",
        "cnn_model.add(Conv2D(filters=128,kernel_size= (3,3), padding = 'same', activation = 'elu', data_format='channels_first',input_shape=(3, 32,32),use_bias = True, kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "cnn_model.add(BatchNormalization())\n",
        "cnn_model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "cnn_model.add(Dropout(0.35))\n",
        "\n",
        "cnn_model.add(Conv2D(filters=128,kernel_size= (1,1), padding = 'same', activation = 'elu', data_format='channels_first',input_shape=(3, 32,32),use_bias = True, kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "cnn_model.add(BatchNormalization())\n",
        "cnn_model.add(Conv2D(filters=128,kernel_size= (1,1), padding = 'same', activation = 'elu', data_format='channels_first',input_shape=(3, 32,32),use_bias = True, kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "cnn_model.add(BatchNormalization())\n",
        "cnn_model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "cnn_model.add(Dropout(0.4))\n",
        "\n",
        "\n",
        "cnn_model.add(Conv2D(filters=128,kernel_size= (3,3), padding = 'same', activation = 'elu', data_format='channels_first',input_shape=(3, 32,32),use_bias = True, kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "cnn_model.add(BatchNormalization())\n",
        "cnn_model.add(Conv2D(filters=128,kernel_size= (3,3), padding = 'same', activation = 'elu', data_format='channels_first',input_shape=(3, 32,32),use_bias = True, kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "cnn_model.add(BatchNormalization())\n",
        "cnn_model.add(Dropout(0.5))\n",
        "\n",
        "cnn_model.add(Conv2D(filters=128,kernel_size= (3,3), padding = 'same', activation = 'elu', data_format='channels_first',input_shape=(3, 32,32),use_bias = True, kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "cnn_model.add(BatchNormalization())\n",
        "cnn_model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "cnn_model.add(Dropout(0.4))\n",
        "\n",
        "cnn_model.add(Flatten())\n",
        "cnn_model.add(Dense(10))\n",
        "cnn_model.add(Activation('softmax'))\n",
        "\n",
        "# Deciding optimiser for backprop\n",
        "opt = keras.optimizers.Adam()\n",
        "# Defining backprop\n",
        "cnn_model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
        "\n",
        "cnn_model.fit(Xtr,Ytr,batch_size=256, epochs=100,validation_split=0.05, shuffle= True)\n",
        "\n",
        "Ytest = cnn_model.predict(Xtest)\n",
        "\n",
        "Ytest = np.argmax(Ytest,axis=1)\n",
        "pred = []\n",
        "print(pred)\n",
        "for i in range(len(Ytest)):\n",
        "    pred.append(str(Ytest[i]))\n",
        "file = open('output.txt','w')\n",
        "for i in pred:\n",
        "    file.write((i)+'\\n')\n",
        "file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCriEDRNKVAf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}